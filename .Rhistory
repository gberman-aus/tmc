remove_url = TRUE,
remove_separators = TRUE)
# remove stopwords
tokens_df <- tokens_select(tokens_df,
pattern = stopwords("en"),
selection = "remove")
# if using n-grams, create the n-grams and update the tokens
if (configs$`document-term_matrix_options`$`n-grams` > 1){
tokens_df <- tokens_ngrams(tokens_df, n = 1:(configs$`document-term_matrix_options`$`n-grams`+1))
print("Creating n-grams.")
} else {print("No n-grams created.")}
# Load the .yml files
configs <- read_yaml('configuration/corpus_config.yml')
# Load the .csv file
df <- read.csv('data/intermediate/code_extracted.csv')
names(df)
# Convert it to a corpus
corp_df <- corpus(df, text_field = "TextNoCode")
docnames(corp_df) <- df$Id
# Add metadata
meta_data <- configs$corpus_format$data_columns
for (metas in meta_data){
docvars(corp_df, metas) <- names(df$metas)
}
# save the corpus
saveRDS(corp_df, 'data/intermediate/corpus.RDS')
# create tokens
tokens_df <- tokens(corp_df,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# remove stopwords
tokens_df <- tokens_select(tokens_df,
pattern = stopwords("en"),
selection = "remove")
# if using n-grams, create the n-grams and update the tokens
if (configs$`document-term_matrix_options`$`n-grams` > 1){
tokens_df <- tokens_ngrams(tokens_df, n = 1:(configs$`document-term_matrix_options`$`n-grams`+1))
print("Creating n-grams.")
} else {print("No n-grams created.")}
# save the tokens
saveRDS(tokens_df, 'data/intermediate/tokens.RDS')
# create document frequency matrix
dfm_df_count <- dfm(tokens_df)
topfeatures(dfm_df_count)
# Requirements
library(yaml)
library(quanteda)
library(readtext)
# make sure we are in the right directory
setwd('//home/gberman/github/tmc')
# Load the .yml files
configs <- read_yaml('configuration/corpus_config.yml')
# Load the .csv file
df <- read.csv('data/intermediate/code_extracted.csv')
names(df)
# Convert it to a corpus
corp_df <- corpus(df, text_field = "TextNoCode")
docnames(corp_df) <- df$Id
# Add metadata
meta_data <- configs$corpus_format$data_columns
for (metas in meta_data){
docvars(corp_df, metas) <- names(df$metas)
}
# save the corpus
saveRDS(corp_df, 'data/intermediate/corpus.RDS')
# create tokens
tokens_df <- tokens(corp_df,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# remove stopwords
tokens_df <- tokens_select(tokens_df,
pattern = stopwords("en"),
selection = "remove")
# if using n-grams, create the n-grams and update the tokens
if (configs$`document-term_matrix_options`$`n-grams` > 1){
tokens_df <- tokens_ngrams(tokens_df, n = 1:(configs$`document-term_matrix_options`$`n-grams`+1))
print("Creating n-grams.")
} else {print("No n-grams created.")}
# save the tokens
saveRDS(tokens_df, 'data/intermediate/tokens.RDS')
# create document frequency matrix
dfm_df_count <- dfm(tokens_df)
dfm_df_count <- dfm_trim(dfm_df_count,
min_termfreq = configs$`document-term_matrix_options`$min_term_frequency)
dfm_df_count <- dfm_trim(dfm_df_count,
max_docfreq = configs$`document-term_matrix_options`$max_doc_frquency, docfreq_type = "prop")
dfm_df_tfidf <- dfm_tfidf(dfm_df_count)
# save the document frequency matrix
saveRDS(dfm_df_count, 'data/intermediate/dfm_count.RDS')
saveRDS(dfm_df_tfidf, 'data/intermediate/dfm_tfidf.RDS')
# clean up
rm(configs, df, dfm_df_count, dfm_df_tfidf, tokens_df , corp_df, meta_data, metas)
rm(tokens_df_grams)
# Requirements
library(yaml)
library(stm)
# make sure we are in the right directory
setwd('//home/gberman/github/tmc')
# Load the .yml files
configs <- read_yaml('configuration/corpus_config.yml')
# Load the .RDS dfm file
if (configs$`document-term_matrix_options`$dtm_method == 1) {
dtm <- readRDS('data/intermediate/dfm_count.RDS')
} else {
dtm <- readRDS('data/intermediate/dfm_tfidf.RDS')
}
topfeatures(dtm)
# Requirements
library(yaml)
library(quanteda)
library(readtext)
# make sure we are in the right directory
setwd('//home/gberman/github/tmc')
# Load the .yml files
configs <- read_yaml('configuration/corpus_config.yml')
# Load the .csv file
df <- read.csv('data/intermediate/code_extracted.csv')
names(df)
# Convert it to a corpus
corp_df <- corpus(df, text_field = "TextNoCode")
docnames(corp_df) <- df$Id
# Add metadata
meta_data <- configs$corpus_format$data_columns
for (metas in meta_data){
docvars(corp_df, metas) <- names(df$metas)
}
# save the corpus
saveRDS(corp_df, 'data/intermediate/corpus.RDS')
# create tokens
tokens_df <- tokens(corp_df,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# remove stopwords
tokens_df <- tokens_select(tokens_df,
pattern = stopwords("en"),
selection = "remove")
# if using n-grams, create the n-grams and update the tokens
if (configs$`document-term_matrix_options`$`n-grams` > 1){
tokens_df <- tokens_ngrams(tokens_df, n = 1:(configs$`document-term_matrix_options`$`n-grams`+1))
print("Creating n-grams.")
} else {print("No n-grams created.")}
# save the tokens
saveRDS(tokens_df, 'data/intermediate/tokens.RDS')
# create document frequency matrix
dfm_df_count <- dfm(tokens_df)
dfm_df_count <- dfm_trim(dfm_df_count,
min_termfreq = configs$`document-term_matrix_options`$min_term_frequency)
dfm_df_count <- dfm_trim(dfm_df_count,
max_docfreq = configs$`document-term_matrix_options`$max_doc_frquency, docfreq_type = "prop")
dfm_df_tfidf <- dfm_tfidf(dfm_df_count)
# save the document frequency matrix
saveRDS(dfm_df_count, 'data/processed/dfm_count.RDS')
saveRDS(dfm_df_tfidf, 'data/processed/dfm_tfidf.RDS')
# clean up
rm(configs, df, dfm_df_count, dfm_df_tfidf, tokens_df , corp_df, meta_data, metas)
# Requirements
library(yaml)
library(stm)
# make sure we are in the right directory
setwd('//home/gberman/github/tmc')
# Load the .yml files
configs <- read_yaml('configuration/corpus_config.yml')
# Load the .RDS dfm file
if (configs$`document-term_matrix_options`$dtm_method == 1) {
dtm <- readRDS('data/processed/dfm_count.RDS')
} else {
dtm <- readRDS('data/processed/dfm_tfidf.RDS')
}
dtm <- convert(dtm, to = "stm")
View(dtm)
testing <- stm(documents = dtm$documents, vocab = dtm$vocab, K = 10, prevalence =~ CreationYear, max.em.its = 75, data = dtm$meta, init.type = "Spectral")
testing <- stm(documents = dtm$documents, vocab = dtm$vocab, K = 10, max.em.its = 75, data = dtm$meta, init.type = "Spectral")
summary(testing)
# Load the .yml files
configs <- read_yaml('configuration/corpus_config.yml')
# Load the .csv file
df <- read.csv('data/intermediate/code_extracted.csv')
names(df)
# Convert it to a corpus
corp_df <- corpus(df, text_field = "TextNoCode")
docnames(corp_df) <- df$Id
# Add metadata
meta_data <- configs$corpus_format$data_columns
for (metas in meta_data){
docvars(corp_df, metas) <- names(df$metas)
}
# save the corpus
saveRDS(corp_df, 'data/intermediate/corpus.RDS')
# create tokens
tokens_df <- tokens(corp_df,
remove_punct = TRUE,
remove_numbers = FALSE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# remove stopwords
tokens_df <- tokens_select(tokens_df,
pattern = stopwords("en"),
selection = "remove")
topfeatures(tokens_df)
# if stemming, do that
if (configs$`document-term_matrix_options`$stemming == 1){
tokens_df <- tokens_wordstem(tokens_df)
print("Stemming.")
} else {print("No stemming.")}
# if using n-grams, create the n-grams and update the tokens
if (configs$`document-term_matrix_options`$`n-grams` > 1){
tokens_df <- tokens_ngrams(tokens_df, n = 1:(configs$`document-term_matrix_options`$`n-grams`+1))
print("Creating n-grams.")
} else {print("No n-grams created.")}
# create document frequency matrix
dfm_df_count <- dfm(tokens_df)
dfm_df_count <- dfm_trim(dfm_df_count,
min_termfreq = configs$`document-term_matrix_options`$min_term_frequency)
dfm_df_count <- dfm_trim(dfm_df_count,
max_docfreq = configs$`document-term_matrix_options`$max_doc_frquency, docfreq_type = "prop")
dfm_df_tfidf <- dfm_tfidf(dfm_df_count)
# save the document frequency matrix
saveRDS(dfm_df_count, 'data/processed/dfm_count.RDS')
saveRDS(dfm_df_tfidf, 'data/processed/dfm_tfidf.RDS')
topfeatures(dfm_df_count)
# clean up
rm(configs, df, dfm_df_count, dfm_df_tfidf, tokens_df , corp_df, meta_data, metas)
# Requirements
library(yaml)
library(quanteda)
library(readtext)
# make sure we are in the right directory
setwd('//home/gberman/github/tmc')
# Load the .yml files
configs <- read_yaml('configuration/corpus_config.yml')
# Load the .csv file
df <- read.csv('data/intermediate/code_extracted.csv')
names(df)
# Convert it to a corpus
corp_df <- corpus(df, text_field = "TextNoCode")
docnames(corp_df) <- df$Id
# Add metadata
meta_data <- configs$corpus_format$data_columns
for (metas in meta_data){
docvars(corp_df, metas) <- names(df$metas)
}
# save the corpus
saveRDS(corp_df, 'data/intermediate/corpus.RDS')
# create tokens
tokens_df <- tokens(corp_df,
remove_punct = TRUE,
remove_numbers = FALSE,
remove_symbols = TRUE,
remove_url = TRUE,
remove_separators = TRUE)
# remove stopwords
tokens_df <- tokens_select(tokens_df,
pattern = stopwords("en"),
selection = "remove")
# if stemming, do that
if (configs$`document-term_matrix_options`$stemming == 1){
tokens_df <- tokens_wordstem(tokens_df)
print("Stemming.")
} else {print("No stemming.")}
# if using n-grams, create the n-grams and update the tokens
if (configs$`document-term_matrix_options`$`n-grams` > 1){
tokens_df <- tokens_ngrams(tokens_df, n = 1:(configs$`document-term_matrix_options`$`n-grams`+1))
print("Creating n-grams.")
} else {print("No n-grams created.")}
# save the tokens
saveRDS(tokens_df, 'data/intermediate/tokens.RDS')
# create document frequency matrix
dfm_df <- dfm(tokens_df)
dfm_df <- dfm_trim(dfm_df,
min_termfreq = configs$`document-term_matrix_options`$min_term_frequency)
dfm_df <- dfm_trim(dfm_df,
max_docfreq = configs$`document-term_matrix_options`$max_doc_frquency, docfreq_type = "prop")
if (configs$`document-term_matrix_options`$dtm_method == 2) {
dfm_df <- dfm_tfidf(dfm_df)
print("Using term frequency-inverse document frequency.")
} else {
print("Using term occurence.")
}
# save the document frequency matrix
saveRDS(dfm_df, 'data/processed/dfm.RDS')
# clean up
rm(configs, df, dfm_df, tokens_df , corp_df, meta_data, metas)
# Requirements
library(yaml)
library(stm)
# make sure we are in the right directory
setwd('//home/gberman/github/tmc')
# Load the .yml files
configs <- read_yaml('configuration/corpus_config.yml')
# Load the .RDS dfm file
dtm <- readRDS('data/processed/dfm.RDS')
# convert the dfm format for the stm package
dtm <- convert(dtm, to = "stm")
trainings <- read_yaml('configuration/training_config.yml')
training_plan <- read_yaml('configuration/training_config.yml')
rm(trainings)
# search for the best number of topics
topic_range <- seq(from = training_plan$K_value_range$min_K, to = training_plan$K_value_range$max_K, by = training_plan$K_value_range$K_interval)
# Requirements
library(yaml)
library(stm)
# make sure we are in the right directory
setwd('//home/gberman/github/tmc')
# Load the .yml files
configs <- read_yaml('configuration/corpus_config.yml')
training_plan <- read_yaml('configuration/training_config.yml')
# Load the .RDS dfm file
dtm <- readRDS('data/processed/dfm.RDS')
# convert the dfm format for the stm package
dtm <- convert(dtm, to = "stm")
# search for the best number of topics
topic_range <- seq(from = training_plan$K_value_range$min_K, to = training_plan$K_value_range$max_K, by = training_plan$K_value_range$K_interval)
ManyTop<- manyTopics(documents = dtm$documents,
vocab = dtm$vocab,
K = topic_range,
max.em.its = 100,
data = dtm$meta,
init.type = "Spectral",
seed=180587,
runs=50)
saveRDS(ManyTop, 'results/models/manytopics.RDS')
# identify data relevant topics
dtopics <- list()
for(i in 1:length(topic_range)){
dtopics[[i]] <- findTopic(ManyTop$out[[i]], n = 15, c("data"))
}
# Plot Topic Summaries
for(i in 1:length(topic_range)) {
pdf(file = paste0("plots/SummaryMT", nameX[i], ".pdf"), width = 12, height = 8)
plot(ManyTop$out[[i]], xlim = c(0,0.1), text.cex = 0.5, n = 15,
main = paste0(nameX[i], " ", "Topics"))
dev.off()
}
# Plot Topic Summaries
for(i in 1:length(topic_range)) {
pdf(file = paste0("plots/SummaryMT", topic_range[i], ".pdf"), width = 12, height = 8)
plot(ManyTop$out[[i]], xlim = c(0,0.1), text.cex = 0.5, n = 15,
main = paste0(nameX[i], " ", "Topics"))
dev.off()
}
# Plot Topic Summaries
for(i in 1:length(topic_range)) {
pdf(file = paste0("results/outputs", topic_range[i], ".pdf"), width = 12, height = 8)
plot(ManyTop$out[[i]], xlim = c(0,0.1), text.cex = 0.5, n = 15,
main = paste0(topic_range[i], " ", "Topics"))
dev.off()
}
# Plot Topic Summaries
for(i in 1:length(topic_range)) {
pdf(file = paste0("results/outputs/", topic_range[i], ".pdf"), width = 12, height = 8)
plot(ManyTop$out[[i]], xlim = c(0,0.1), text.cex = 0.5, n = 15,
main = paste0(topic_range[i], " ", "Topics"))
dev.off()
}
# identify data relevant topics
dtopics <- list()
for(i in 1:length(topic_range)){
dtopics[[i]] <- findTopic(ManyTop$out[[i]], n = 15, c("machine"))
}
# identify data relevant topics
dtopics <- list()
for(i in 1:length(topic_range)){
dtopics[[i]] <- findTopic(ManyTop$out[[i]], n = 15, c("jupyt"))
}
# Plot Topic Summaries
for(i in 1:length(topic_range)) {
pdf(file = paste0("results/outputs/", topic_range[i], ".pdf"), width = 12, height = 8)
plot(ManyTop$out[[i]], xlim = c(0,0.1), text.cex = 0.5, n = 15,
main = paste0(topic_range[i], " ", "Topics"))
dev.off()
}
# Plot Topic Summaries
for(i in 1:length(topic_range)) {
pdf(file = paste0("results/outputs/", topic_range[i], ".pdf"), width = 12, height = 8)
plot(ManyTop$out[[i]],text.cex = 0.5, n = 15,
main = paste0(topic_range[i], " ", "Topics"))
dev.off()
}
# make TC01 TC05, Topic Correlation Networks list
TC01 <- list()
TC05 <- list()
for(i in 1:length(topic_range)) {
TC01[[i]] <- topicCorr(ManyTop$out[[i]], method = "simple", cutoff = 0.01)
TC05[[i]] <- topicCorr(ManyTop$out[[i]], method = "simple", cutoff = 0.05)
}
# make igraph igTCO1 igTC05 list for plotting
igTC01 <-list()
igTC05 <-list()
for(i in 1:length(topic_range)) {
igTC01[[i]] <- graph_from_adjacency_matrix(TC01[[i]]$poscor, mode = "undirected",
weighted = TRUE, diag = FALSE)
igTC05[[i]] <- graph_from_adjacency_matrix(TC05[[i]]$poscor, mode = "undirected",
weighted = TRUE, diag = FALSE)
# set edge width
E(igTC01[[i]])$width <- E(igTC01[[i]])$weight*50
E(igTC05[[i]])$width <- E(igTC05[[i]])$weight*50
# side by side plot of networks and export as pdf
pdf(file = paste0("results/outputs/TC0105", topic_range[i], "fr.pdf"), width = 12, height = 8)
par(mfrow = c(1,2))
plot(igTC01[[i]], layout = layout_with_fr,
vertex.size = degree(igTC01[[i]], mode = "all")*0.5,
main = paste0(topic_range[i], " ", "Topics Correlation Network (cutoff = 0.01)")
)
plot(igTC05[[i]], layout = layout_with_fr,
vertex.size = degree(igTC05[[i]], mode = "all")*1.5,
main = paste0(topic_range[i], " ", "Topics Correlation Network (cutoff = 0.05)")
)
dev.off()
# reset edge width for 4 up plot
E(igTC01[[i]])$width <- E(igTC01[[i]])$weight*10
E(igTC05[[i]])$width <- E(igTC05[[i]])$weight*10
# 4 up community detection (using NG and GO) plot and export as pdf
pdf(file = paste0("results/outputs/TCCD0105", topic_range[i], "fr.pdf"), width = 12, height = 8)
par(mfrow = c(2,2))
# plot community detection using NG cutoff = 0.01
plot(cluster_edge_betweenness(igTC01[[i]]), igTC01[[i]],
vertex.size = degree(igTC01[[i]], mode = "all")*0.5,
main = paste0(topic_range[i], " ", "Topics Community Detection (Newman-Girvan, cutoff = 0.01)"))
# plot community detection using NG cutoff = 0.05
plot(cluster_edge_betweenness(igTC05[[i]]), igTC05[[i]],
vertex.size = degree(igTC05[[i]], mode = "all")*1.5,
main = paste0(topic_range[i], " ", "Topics Community Detection (Newman-Girvan, cutoff = 0.05)"))
# plot community detection using GO cutoff = 0.01
plot(cluster_fast_greedy(as.undirected(igTC01[[i]])), as.undirected(igTC01[[i]]),
vertex.size = degree(igTC01[[i]], mode = "all")*0.5,
main = paste0(topic_range[i], " ", "Topics Community Detection (Greedy-Optimisation, cutoff = 0.01)"))
# plot community detection using GO cutoff = 0.05
plot(cluster_fast_greedy(as.undirected(igTC05[[i]])), as.undirected(igTC05[[i]]),
vertex.size = degree(igTC05[[i]], mode = "all")*1.5,
main = paste0(topic_range[i], " ", "Topics Community Detection (Greedy-Optimisation, cutoff = 0.05)"))
dev.off()
}
install.packages("igraph")
library(igraph)
renv::snapshot()
rm(dtopics)
for(i in 1:length(topic_range)) {
igTC01[[i]] <- graph_from_adjacency_matrix(TC01[[i]]$poscor, mode = "undirected",
weighted = TRUE, diag = FALSE)
igTC05[[i]] <- graph_from_adjacency_matrix(TC05[[i]]$poscor, mode = "undirected",
weighted = TRUE, diag = FALSE)
# set edge width
E(igTC01[[i]])$width <- E(igTC01[[i]])$weight*50
E(igTC05[[i]])$width <- E(igTC05[[i]])$weight*50
# side by side plot of networks and export as pdf
pdf(file = paste0("results/outputs/TC0105", topic_range[i], "fr.pdf"), width = 12, height = 8)
par(mfrow = c(1,2))
plot(igTC01[[i]], layout = layout_with_fr,
vertex.size = degree(igTC01[[i]], mode = "all")*0.5,
main = paste0(topic_range[i], " ", "Topics Correlation Network (cutoff = 0.01)")
)
plot(igTC05[[i]], layout = layout_with_fr,
vertex.size = degree(igTC05[[i]], mode = "all")*1.5,
main = paste0(topic_range[i], " ", "Topics Correlation Network (cutoff = 0.05)")
)
dev.off()
# reset edge width for 4 up plot
E(igTC01[[i]])$width <- E(igTC01[[i]])$weight*10
E(igTC05[[i]])$width <- E(igTC05[[i]])$weight*10
# 4 up community detection (using NG and GO) plot and export as pdf
pdf(file = paste0("results/outputs/TCCD0105", topic_range[i], "fr.pdf"), width = 12, height = 8)
par(mfrow = c(2,2))
# plot community detection using NG cutoff = 0.01
plot(cluster_edge_betweenness(igTC01[[i]]), igTC01[[i]],
vertex.size = degree(igTC01[[i]], mode = "all")*0.5,
main = paste0(topic_range[i], " ", "Topics Community Detection (Newman-Girvan, cutoff = 0.01)"))
# plot community detection using NG cutoff = 0.05
plot(cluster_edge_betweenness(igTC05[[i]]), igTC05[[i]],
vertex.size = degree(igTC05[[i]], mode = "all")*1.5,
main = paste0(topic_range[i], " ", "Topics Community Detection (Newman-Girvan, cutoff = 0.05)"))
# plot community detection using GO cutoff = 0.01
plot(cluster_fast_greedy(as.undirected(igTC01[[i]])), as.undirected(igTC01[[i]]),
vertex.size = degree(igTC01[[i]], mode = "all")*0.5,
main = paste0(topic_range[i], " ", "Topics Community Detection (Greedy-Optimisation, cutoff = 0.01)"))
# plot community detection using GO cutoff = 0.05
plot(cluster_fast_greedy(as.undirected(igTC05[[i]])), as.undirected(igTC05[[i]]),
vertex.size = degree(igTC05[[i]], mode = "all")*1.5,
main = paste0(topic_range[i], " ", "Topics Community Detection (Greedy-Optimisation, cutoff = 0.05)"))
dev.off()
}
warnings()
rm(testing)
#save an R image
save.image("results/outputs/manytopics.RData")
#clean up
rm(configs, dtm, training_plan, topic_range, ManyTop, igTC01, igTC05, TC01, TC05)
rm(i)
packageVersion('renv')
print("testing R")
?renv
renv::clean()
renv::snapshot()
library(yaml)
installed.packages('yaml')
install.packages('yaml')
renv::snapshot()
renv::init()
renv::init()
renv::update()
renv::snapshot()
renv::repair()
renv::status()
